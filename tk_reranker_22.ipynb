{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aacbc69f",
   "metadata": {},
   "source": [
    "# Transformer-Kernel (TK) Reranker Pipeline\n",
    "Includes robust file checks, BM25 candidates, TK model fine-tuning with scheduler, and detailed logging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b4491139",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kartikarya/.pyenv/versions/3.13.2/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "from transformers import AutoTokenizer, AutoModel, get_linear_schedule_with_warmup\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import pickle\n",
    "from collections import defaultdict\n",
    "import math\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "tokenizer = AutoTokenizer.from_pretrained('allenai/scibert_scivocab_uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3a1bc48f-1f9f-4e5b-9f51-e35f8b21f6e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a5ba2f2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data - Traditional IR BM25 Embeddings\n",
    "df_query_train = pd.read_pickle(\"df_query_train_top100.pkl\")\n",
    "df_query_dev   = pd.read_pickle(\"df_query_dev_top100.pkl\")\n",
    "df_query_test  = pd.read_pickle(\"df_query_test_top100.pkl\")\n",
    "\n",
    "df_collection = pd.read_pickle(\"subtask_4b/subtask4b_collection_data.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f8a567b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['post_id', 'tweet_text', 'cord_uid', 'normalized_tweet_text', 'cleaned_tweet_text', 'final_query', 'bm25_topk', 'in_topx']\n"
     ]
    }
   ],
   "source": [
    "# print(df_collection.columns.tolist())\n",
    "print(df_query_train.columns.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f921365",
   "metadata": {},
   "source": [
    "## TK - Model Definition\n",
    "\n",
    "- Loads SciBERT (`AutoModel`)\n",
    "- Defines Gaussian kernels\n",
    "- Implements `forward(q_input, d_input)` returning a score tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7427eb4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TKReRanker(nn.Module):\n",
    "    def __init__(self, model_name='allenai/scibert_scivocab_uncased', num_kernels=11):\n",
    "        super().__init__()\n",
    "        self.bert = AutoModel.from_pretrained(model_name)\n",
    "        self.kernels = self._init_kernels(num_kernels)\n",
    "        self.linear = nn.Linear(num_kernels, 1)\n",
    "\n",
    "    def _init_kernels(self, num_kernels):\n",
    "        mus = torch.linspace(-1, 1, steps=num_kernels)\n",
    "        sigmas = torch.full((num_kernels,), 0.1)\n",
    "        return nn.ParameterDict({\n",
    "            'mus': nn.Parameter(mus, requires_grad=False),\n",
    "            'sigmas': nn.Parameter(sigmas, requires_grad=False)\n",
    "        })\n",
    "\n",
    "    def forward(self, q_input, d_input):\n",
    "        q_vecs = self.bert(**q_input).last_hidden_state\n",
    "        d_vecs = self.bert(**d_input).last_hidden_state\n",
    "\n",
    "        q_norm = F.normalize(q_vecs, p=2, dim=-1)\n",
    "        d_norm = F.normalize(d_vecs, p=2, dim=-1)\n",
    "        sim = torch.bmm(q_norm, d_norm.transpose(1, 2))  # [B, Q, D]\n",
    "\n",
    "        batch_kernels = []\n",
    "        for mu, sigma in zip(self.kernels['mus'], self.kernels['sigmas']):\n",
    "            kernel = torch.exp(-((sim - mu)**2) / (2 * sigma**2))\n",
    "            log_pool = torch.log(torch.clamp(kernel.sum(dim=2), min=1e-8))\n",
    "            batch_kernels.append(log_pool.sum(dim=1))\n",
    "\n",
    "        K = torch.stack(batch_kernels, dim=1)\n",
    "        return self.linear(K).squeeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "25e4fc80",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TKTripletDataset(Dataset):\n",
    "    def __init__(self, df_q, df_coll, tokenizer, num_negatives=1, doc_field='abstract'):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.samples = []\n",
    "        coll_text = df_coll.set_index('cord_uid')[doc_field].to_dict()\n",
    "        for _, r in df_q.iterrows():\n",
    "            q, pos_uid = r['final_query'], r['cord_uid']\n",
    "            pos_txt = coll_text[pos_uid]\n",
    "            negs = [uid for uid in r['bm25_topk'] if uid != pos_uid]\n",
    "            for _ in range(num_negatives):\n",
    "                neg_uid = random.choice(negs)\n",
    "                self.samples.append((q, pos_txt, coll_text[neg_uid]))\n",
    "    def __len__(self): return len(self.samples)\n",
    "    def __getitem__(self, idx): return self.samples[idx]\n",
    "\n",
    "def collate_tk(batch):\n",
    "    claims, pos_docs, neg_docs = zip(*batch)\n",
    "    labels = torch.zeros(len(batch), dtype=torch.float)\n",
    "\n",
    "    q_tok = tokenizer(list(claims), padding=True, truncation=True, max_length=512, return_tensors='pt' )\n",
    "    p_tok = tokenizer(list(pos_docs), padding=True, truncation=True, max_length=512, return_tensors='pt')\n",
    "    n_tok = tokenizer(list(neg_docs), padding=True, truncation=True, max_length=512, return_tensors='pt')\n",
    "\n",
    "    return q_tok, p_tok, n_tok, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "67f33d98-eb21-4c3e-add6-43fd5a4d9379",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rerank_batch(df_q, df_coll, model, tokenizer, prerank_col='bm25_topk', query_col='final_query', \n",
    "                 doc_id_col='cord_uid', doc_field='abstract', rerank_k=10, device=device):\n",
    "\n",
    "    model.eval()\n",
    "    reranked = []\n",
    "\n",
    "    # build a quick lookup for doc text\n",
    "    coll_text = df_coll.set_index(doc_id_col)[doc_field].to_dict()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for q_text, cands in zip(df_q[query_col], df_q[prerank_col]):\n",
    "            scores = []\n",
    "            # only consider the top-N from prerank\n",
    "            for uid in cands:\n",
    "                doc_text = coll_text[uid]\n",
    "\n",
    "                # tokenize & move to device\n",
    "                q_tok = tokenizer(q_text, padding=True, truncation=True, max_length=512, return_tensors='pt').to(device)\n",
    "                d_tok = tokenizer(doc_text, padding=True, truncation=True, max_length=512, return_tensors='pt').to(device)\n",
    "\n",
    "                # score\n",
    "                s = model(q_tok, d_tok).item()\n",
    "                scores.append((uid, s))\n",
    "\n",
    "            # sort by score descending, take top rerank_k\n",
    "            topk = [uid for uid, _ in sorted(scores, key=lambda x: x[1], reverse=True)[:rerank_k]]\n",
    "            reranked.append(topk)\n",
    "\n",
    "    return reranked\n",
    "\n",
    "def get_performance_mrr(data, col_gold, col_pred, list_k=[1,5,10]):\n",
    "    perf = {}\n",
    "    for k in list_k:\n",
    "        data['in_topx'] = data.apply(\n",
    "            lambda x: 1/([i for i in x[col_pred][:k]].index(x[col_gold])+1)\n",
    "                    if x[col_gold] in x[col_pred][:k] else 0,\n",
    "            axis=1\n",
    "        )\n",
    "        perf[k] = data['in_topx'].mean()\n",
    "    return perf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "252ce797",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/y_/1fphxzpx3bgflh5ffkzptcm00000gp/T/ipykernel_89257/2282966465.py:31: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler()\n",
      "/Users/kartikarya/.pyenv/versions/3.13.2/lib/python3.13/site-packages/torch/amp/grad_scaler.py:136: UserWarning: torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\n",
      "  warnings.warn(\n",
      "/Users/kartikarya/.pyenv/versions/3.13.2/lib/python3.13/site-packages/torch/utils/data/dataloader.py:683: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "Traceback (most recent call last):\n",
      "  File \u001b[35m\"<string>\"\u001b[0m, line \u001b[35m1\u001b[0m, in \u001b[35m<module>\u001b[0m\n",
      "    from multiprocessing.spawn import spawn_main; \u001b[31mspawn_main\u001b[0m\u001b[1;31m(tracker_fd=82, pipe_handle=96)\u001b[0m\n",
      "                                                  \u001b[31m~~~~~~~~~~\u001b[0m\u001b[1;31m^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[0m\n",
      "  File \u001b[35m\"/Users/kartikarya/.pyenv/versions/3.13.2/lib/python3.13/multiprocessing/spawn.py\"\u001b[0m, line \u001b[35m122\u001b[0m, in \u001b[35mspawn_main\u001b[0m\n",
      "    exitcode = _main(fd, parent_sentinel)\n",
      "  File \u001b[35m\"/Users/kartikarya/.pyenv/versions/3.13.2/lib/python3.13/multiprocessing/spawn.py\"\u001b[0m, line \u001b[35m132\u001b[0m, in \u001b[35m_main\u001b[0m\n",
      "    self = reduction.pickle.load(from_parent)\n",
      "\u001b[1;35mAttributeError\u001b[0m: \u001b[35mCan't get attribute 'TKTripletDataset' on <module '__main__' (<class '_frozen_importlib.BuiltinImporter'>)>\u001b[0m\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 37\u001b[39m\n\u001b[32m     34\u001b[39m model.train()\n\u001b[32m     35\u001b[39m epoch_loss = \u001b[32m0.0\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m37\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m step, (q_tok, p_tok, n_tok, _) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[32m     38\u001b[39m     \u001b[38;5;66;03m# ─── Move everything to GPU here ───\u001b[39;00m\n\u001b[32m     39\u001b[39m     q_tok = {k: v.to(device) \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m q_tok.items()}\n\u001b[32m     40\u001b[39m     p_tok = {k: v.to(device) \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m p_tok.items()}\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.13.2/lib/python3.13/site-packages/torch/utils/data/dataloader.py:493\u001b[39m, in \u001b[36mDataLoader.__iter__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    491\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._iterator\n\u001b[32m    492\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m493\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_get_iterator\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.13.2/lib/python3.13/site-packages/torch/utils/data/dataloader.py:424\u001b[39m, in \u001b[36mDataLoader._get_iterator\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    422\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    423\u001b[39m     \u001b[38;5;28mself\u001b[39m.check_worker_number_rationality()\n\u001b[32m--> \u001b[39m\u001b[32m424\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_MultiProcessingDataLoaderIter\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.13.2/lib/python3.13/site-packages/torch/utils/data/dataloader.py:1171\u001b[39m, in \u001b[36m_MultiProcessingDataLoaderIter.__init__\u001b[39m\u001b[34m(self, loader)\u001b[39m\n\u001b[32m   1164\u001b[39m w.daemon = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m   1165\u001b[39m \u001b[38;5;66;03m# NB: Process.start() actually take some time as it needs to\u001b[39;00m\n\u001b[32m   1166\u001b[39m \u001b[38;5;66;03m#     start a process and pass the arguments over via a pipe.\u001b[39;00m\n\u001b[32m   1167\u001b[39m \u001b[38;5;66;03m#     Therefore, we only add a worker to self._workers list after\u001b[39;00m\n\u001b[32m   1168\u001b[39m \u001b[38;5;66;03m#     it started, so that we do not call .join() if program dies\u001b[39;00m\n\u001b[32m   1169\u001b[39m \u001b[38;5;66;03m#     before it starts, and __del__ tries to join but will get:\u001b[39;00m\n\u001b[32m   1170\u001b[39m \u001b[38;5;66;03m#     AssertionError: can only join a started process.\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1171\u001b[39m \u001b[43mw\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstart\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1172\u001b[39m \u001b[38;5;28mself\u001b[39m._index_queues.append(index_queue)\n\u001b[32m   1173\u001b[39m \u001b[38;5;28mself\u001b[39m._workers.append(w)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.13.2/lib/python3.13/multiprocessing/process.py:121\u001b[39m, in \u001b[36mBaseProcess.start\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    118\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _current_process._config.get(\u001b[33m'\u001b[39m\u001b[33mdaemon\u001b[39m\u001b[33m'\u001b[39m), \\\n\u001b[32m    119\u001b[39m        \u001b[33m'\u001b[39m\u001b[33mdaemonic processes are not allowed to have children\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m    120\u001b[39m _cleanup()\n\u001b[32m--> \u001b[39m\u001b[32m121\u001b[39m \u001b[38;5;28mself\u001b[39m._popen = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_Popen\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    122\u001b[39m \u001b[38;5;28mself\u001b[39m._sentinel = \u001b[38;5;28mself\u001b[39m._popen.sentinel\n\u001b[32m    123\u001b[39m \u001b[38;5;66;03m# Avoid a refcycle if the target function holds an indirect\u001b[39;00m\n\u001b[32m    124\u001b[39m \u001b[38;5;66;03m# reference to the process object (see bpo-30775)\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.13.2/lib/python3.13/multiprocessing/context.py:224\u001b[39m, in \u001b[36mProcess._Popen\u001b[39m\u001b[34m(process_obj)\u001b[39m\n\u001b[32m    222\u001b[39m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[32m    223\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_Popen\u001b[39m(process_obj):\n\u001b[32m--> \u001b[39m\u001b[32m224\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_default_context\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_context\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mProcess\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_Popen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprocess_obj\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.13.2/lib/python3.13/multiprocessing/context.py:289\u001b[39m, in \u001b[36mSpawnProcess._Popen\u001b[39m\u001b[34m(process_obj)\u001b[39m\n\u001b[32m    286\u001b[39m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[32m    287\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_Popen\u001b[39m(process_obj):\n\u001b[32m    288\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpopen_spawn_posix\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Popen\n\u001b[32m--> \u001b[39m\u001b[32m289\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mPopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprocess_obj\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.13.2/lib/python3.13/multiprocessing/popen_spawn_posix.py:32\u001b[39m, in \u001b[36mPopen.__init__\u001b[39m\u001b[34m(self, process_obj)\u001b[39m\n\u001b[32m     30\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, process_obj):\n\u001b[32m     31\u001b[39m     \u001b[38;5;28mself\u001b[39m._fds = []\n\u001b[32m---> \u001b[39m\u001b[32m32\u001b[39m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mprocess_obj\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.13.2/lib/python3.13/multiprocessing/popen_fork.py:20\u001b[39m, in \u001b[36mPopen.__init__\u001b[39m\u001b[34m(self, process_obj)\u001b[39m\n\u001b[32m     18\u001b[39m \u001b[38;5;28mself\u001b[39m.returncode = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m     19\u001b[39m \u001b[38;5;28mself\u001b[39m.finalizer = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m20\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_launch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprocess_obj\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.13.2/lib/python3.13/multiprocessing/popen_spawn_posix.py:62\u001b[39m, in \u001b[36mPopen._launch\u001b[39m\u001b[34m(self, process_obj)\u001b[39m\n\u001b[32m     60\u001b[39m     \u001b[38;5;28mself\u001b[39m.sentinel = parent_r\n\u001b[32m     61\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(parent_w, \u001b[33m'\u001b[39m\u001b[33mwb\u001b[39m\u001b[33m'\u001b[39m, closefd=\u001b[38;5;28;01mFalse\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[32m---> \u001b[39m\u001b[32m62\u001b[39m         \u001b[43mf\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwrite\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgetbuffer\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     63\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m     64\u001b[39m     fds_to_close = []\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# --- Training with mixed‐precision, scheduler & Dev‐MRR logging ---\n",
    "model = TKReRanker().to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=2e-5, weight_decay=1e-2)\n",
    "epochs = 4\n",
    "\n",
    "# Pre‐tokenization is optional—your collate_tk already does truncation to 512\n",
    "train_ds = TKTripletDataset(df_query_train, df_collection, tokenizer, num_negatives=1)\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_ds,\n",
    "    batch_size=16,         # ↑ larger batch if GPU permits\n",
    "    shuffle=True,\n",
    "    num_workers=4,         # ↑ parallel tokenization\n",
    "    pin_memory=True,       # ↑ faster CPU→GPU copies\n",
    "    collate_fn=collate_tk\n",
    ")\n",
    "\n",
    "total_steps = len(train_loader) * epochs\n",
    "scheduler   = get_linear_schedule_with_warmup(\n",
    "    optimizer,\n",
    "    num_warmup_steps=int(0.1 * total_steps),\n",
    "    num_training_steps=total_steps\n",
    ")\n",
    "\n",
    "criterion    = nn.MarginRankingLoss(margin=0.5)\n",
    "loss_history = []\n",
    "mrr_history  = []\n",
    "best_tk_mrr  = 0.0\n",
    "\n",
    "# initialize mixed‐precision scaler\n",
    "scaler = GradScaler()\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    epoch_loss = 0.0\n",
    "\n",
    "    for step, (q_tok, p_tok, n_tok, _) in enumerate(train_loader):\n",
    "        # ─── Move everything to GPU here ───\n",
    "        q_tok = {k: v.to(device) for k, v in q_tok.items()}\n",
    "        p_tok = {k: v.to(device) for k, v in p_tok.items()}\n",
    "        n_tok = {k: v.to(device) for k, v in n_tok.items()}\n",
    "        \n",
    "        labels = _.to(device)\n",
    "    \n",
    "        optimizer.zero_grad()\n",
    "        with autocast():\n",
    "            s_pos = model(q_tok, p_tok)\n",
    "            s_neg = model(q_tok, n_tok)\n",
    "            target = torch.ones_like(s_pos)\n",
    "            loss   = criterion(s_pos, s_neg, target)\n",
    "\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.unscale_(optimizer)\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        scheduler.step()\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "        if step % 100 == 0:\n",
    "            print(f\"  [Step {step}/{len(train_loader)}]  loss = {loss.item():.4f}\")\n",
    "\n",
    "    avg_loss = epoch_loss / len(train_loader)\n",
    "    loss_history.append(avg_loss)\n",
    "\n",
    "    # --- Dev‐set evaluation ---\n",
    "    model.eval()\n",
    "    tk_dev_top10 = rerank_batch(df_query_dev, df_collection, model, tokenizer)\n",
    "    df_query_dev['tk_top10'] = tk_dev_top10\n",
    "    dev_mrr = get_performance_mrr(df_query_dev, 'cord_uid', 'tk_top10')[5]\n",
    "\n",
    "    print(f\"*** Epoch {epoch} complete. Avg loss = {avg_loss:.4f} | Dev MRR@5 = {dev_mrr:.4f} ***\")\n",
    "\n",
    "    # --- Save best model checkpoint ---\n",
    "    if dev_mrr > best_tk_mrr:\n",
    "        best_tk_mrr = dev_mrr\n",
    "        torch.save(model.state_dict(), \"tk_best.pt\")\n",
    "        print(f\" --- New best Dev MRR@5 = {dev_mrr:.4f} — saved tk_best.pt\")\n",
    "\n",
    "# --- Persist histories for reporting/plotting ---\n",
    "with open(\"tk_loss_history.pkl\", \"wb\") as f:\n",
    "    pickle.dump(loss_history, f)\n",
    "with open(\"tk_mrr_history.pkl\", \"wb\") as f:\n",
    "    pickle.dump(mrr_history, f)\n",
    "\n",
    "print(\"Training done. Best Dev MRR@5 =\", best_tk_mrr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95381926",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Evaluation with logs ---\n",
    "model = TKReRanker().to(device)\n",
    "model.load_state_dict(torch.load('tk_reranker_kartik.pt', map_location=device))\n",
    "model.eval()\n",
    "\n",
    "tk_topk = []\n",
    "for idx, (query, bm25_cands) in enumerate(zip(df_query_test['final_query'], df_query_test['bm25_topk'])):\n",
    "    if idx % 10 == 0:\n",
    "        print(f'Reranking query {idx}/{len(df_query_test)}')\n",
    "    scores = []\n",
    "    for uid in bm25_cands[:100]:\n",
    "        doc = df_collection.loc[df_collection['cord_uid']==uid, 'abstract'].item()\n",
    "        q_tok = tokenizer(query, padding=True, truncation=True, max_length=512, return_tensors='pt').to(device)\n",
    "        d_tok = tokenizer(doc,   padding=True, truncation=True, max_length=512, return_tensors='pt').to(device)\n",
    "        with torch.no_grad(): s = model(q_tok, d_tok).item()\n",
    "        scores.append((uid, s))\n",
    "    ranked = [u for u,_ in sorted(scores, key=lambda x: x[1], reverse=True)[:10]]\n",
    "    tk_topk.append(ranked)\n",
    "df_query_dev['tk_topk'] = tk_topk\n",
    "\n",
    "results = get_performance_mrr(df_query_test, 'cord_uid', 'tk_topk')\n",
    "print('TK Reranker MRR@k:', results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
