{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMSoadAVHFQYw9AtTteh9Qu",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ikc-arya/AIR_SS25/blob/main/Noor_reranking\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "u_8lmAVz0-fV"
      },
      "outputs": [],
      "source": [
        "!pip install -q sentence-transformers pandas scikit-learn"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U sentence-transformers\n",
        "from sentence_transformers import CrossEncoder\n",
        "\n",
        "cross_encoder = CrossEncoder(\"cross-encoder/ms-marco-MiniLM-L-6-v2\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "htQSDKQVHjs0",
        "outputId": "b9152abf-edb6-4525-8e0e-423c2fcfd727"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.11/dist-packages (4.1.0)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (4.51.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (4.67.1)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (2.6.0+cu124)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (1.6.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (1.15.2)\n",
            "Requirement already satisfied: huggingface-hub>=0.20.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (0.30.2)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (11.2.1)\n",
            "Requirement already satisfied: typing_extensions>=4.5.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (4.13.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (3.18.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2025.3.2)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (6.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2.32.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2024.11.6)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.5.3)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->sentence-transformers) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->sentence-transformers) (3.6.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.11.0->sentence-transformers) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2025.1.31)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "!git config --global user.name \"Nooryasser74\"\n",
        "!git config --global user.email \"Noorlasheen135@gmail.com\"\n",
        "\n"
      ],
      "metadata": {
        "id": "dUdAwksTOs7e"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!git credential-manager-core configure\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7SMI5scDTslm",
        "outputId": "6297a302-bff6-40b5-c521-bec4bd044a00"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "git: 'credential-manager-core' is not a git command. See 'git --help'.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git push origin main  # Or 'master' depending on your default branch\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AExNUSNBQRs4",
        "outputId": "c50ed405-bb60-46c0-c4ac-4bd43e7131d4"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: could not read Username for 'https://github.com': No such device or address\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git push origin main\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eBAphwoWQcbH",
        "outputId": "f52f77b5-da02-4d72-8464-a3c5d9e9ece1"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: could not read Username for 'https://github.com': No such device or address\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git config --global user.email \"Noorlasheen135@gmail.com\"\n",
        "!git config --global user.name \"Noor yasser\"\n",
        "\n"
      ],
      "metadata": {
        "id": "55FcvwdMQft_"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!git commit -m \"Noor_Reranking\"\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z3OdavOoRNmF",
        "outputId": "5c36a5c9-e2b2-4501-b1cf-31ee447ccea9"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "On branch main\n",
            "Your branch is up to date with 'origin/main'.\n",
            "\n",
            "nothing to commit, working tree clean\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git commit -m \"Noor_reranking\"\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bskCWeRqRbDu",
        "outputId": "739825a6-fd07-4015-d611-661f3b17697d"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "On branch main\n",
            "Your branch is up to date with 'origin/main'.\n",
            "\n",
            "nothing to commit, working tree clean\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git add .\n"
      ],
      "metadata": {
        "id": "aNxuAvYBRXr-"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!git push origin main  # or 'master'\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3Gz4OXaWRweW",
        "outputId": "be592923-838d-4f3e-876d-9b912815702d"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: could not read Username for 'https://github.com': No such device or address\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q sentence-transformers pandas scikit-learn"
      ],
      "metadata": {
        "id": "Ue4eBZbGQ2p-"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U sentence-transformers\n",
        "from sentence_transformers import CrossEncoder\n",
        "\n",
        "cross_encoder = CrossEncoder(\"cross-encoder/ms-marco-MiniLM-L-6-v2\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DkT9OBTyQ2-J",
        "outputId": "bce72509-1a64-475a-e2d1-6f53baa36eb6"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.11/dist-packages (4.1.0)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (4.51.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (4.67.1)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (2.6.0+cu124)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (1.6.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (1.15.2)\n",
            "Requirement already satisfied: huggingface-hub>=0.20.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (0.30.2)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (11.2.1)\n",
            "Requirement already satisfied: typing_extensions>=4.5.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (4.13.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (3.18.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2025.3.2)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (6.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2.32.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2024.11.6)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.5.3)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->sentence-transformers) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->sentence-transformers) (3.6.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.11.0->sentence-transformers) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2025.1.31)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H0ZYUMYdn6-S",
        "outputId": "1999d0f9-ab50-4da4-e4e2-d85ef4cd55d5"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting datasets\n",
            "  Downloading datasets-3.5.1-py3-none-any.whl.metadata (19 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets) (3.18.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.0.2)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (18.1.0)\n",
            "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
            "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.11/dist-packages (from datasets) (4.67.1)\n",
            "Collecting xxhash (from datasets)\n",
            "  Downloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Collecting multiprocess<0.70.17 (from datasets)\n",
            "  Downloading multiprocess-0.70.16-py311-none-any.whl.metadata (7.2 kB)\n",
            "Collecting fsspec<=2025.3.0,>=2023.1.0 (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets)\n",
            "  Downloading fsspec-2025.3.0-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets) (3.11.15)\n",
            "Requirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.30.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.6.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (6.4.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.20.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24.0->datasets) (4.13.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2025.1.31)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
            "Downloading datasets-3.5.1-py3-none-any.whl (491 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m491.4/491.4 kB\u001b[0m \u001b[31m11.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fsspec-2025.3.0-py3-none-any.whl (193 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m193.6/193.6 kB\u001b[0m \u001b[31m19.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading multiprocess-0.70.16-py311-none-any.whl (143 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.5/143.5 kB\u001b[0m \u001b[31m14.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.8/194.8 kB\u001b[0m \u001b[31m13.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: xxhash, fsspec, dill, multiprocess, datasets\n",
            "  Attempting uninstall: fsspec\n",
            "    Found existing installation: fsspec 2025.3.2\n",
            "    Uninstalling fsspec-2025.3.2:\n",
            "      Successfully uninstalled fsspec-2025.3.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "gcsfs 2025.3.2 requires fsspec==2025.3.2, but you have fsspec 2025.3.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed datasets-3.5.1 dill-0.3.8 fsspec-2025.3.0 multiprocess-0.70.16 xxhash-3.5.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import transformers\n",
        "import numpy as np\n",
        "import torch\n",
        "\n"
      ],
      "metadata": {
        "id": "4xPBOXyo5o5D"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 304
        },
        "id": "CLR3JazEDh4o",
        "outputId": "c8e6c752-dbe3-47bf-ca79-6cab5f0bf3c8"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "mount failed",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-d5df0069828e>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdrive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdrive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36mmount\u001b[0;34m(mountpoint, force_remount, timeout_ms, readonly)\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_ms\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m120000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreadonly\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m   \u001b[0;34m\"\"\"Mount your Google Drive at the specified mountpoint path.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m   return _mount(\n\u001b[0m\u001b[1;32m    101\u001b[0m       \u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m       \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mforce_remount\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36m_mount\u001b[0;34m(mountpoint, force_remount, timeout_ms, ephemeral, readonly)\u001b[0m\n\u001b[1;32m    277\u001b[0m             \u001b[0;34m'https://research.google.com/colaboratory/faq.html#drive-timeout'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    278\u001b[0m         )\n\u001b[0;32m--> 279\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'mount failed'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mextra_reason\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    280\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mcase\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    281\u001b[0m       \u001b[0;31m# Terminate the DriveFS binary before killing bash.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: mount failed"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "i9H2RFYSGS-s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "file_path = '/content/drive/MyDrive/check_that/subtask4b_collection_data.pkl'\n"
      ],
      "metadata": {
        "id": "E3dvsXvuGhej"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " df_collection = pd.read_pickle(file_path)"
      ],
      "metadata": {
        "id": "7IEjMm8sGrrs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "Gwi5OYbNGPt-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "train_file_path = '/content/drive/MyDrive/check_that/subtask4b_query_tweets_train.tsv'\n",
        "df_query_train = pd.read_csv(train_file_path, sep='\\t')\n",
        "dev_file_path = '/content/drive/MyDrive/check_that/subtask4b_query_tweets_dev.tsv'\n",
        "df_query_dev = pd.read_csv(dev_file_path, sep='\\t')"
      ],
      "metadata": {
        "id": "O776RSrVB_Xe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(torch.__version__)\n",
        "print(torch.cuda.is_available())\n",
        "print(\"CUDA available:\", torch.cuda.is_available())\n"
      ],
      "metadata": {
        "id": "qlQU8_pThsiT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_query_train.head()\n"
      ],
      "metadata": {
        "id": "E_3V4U_q5fiC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_query_dev.head()"
      ],
      "metadata": {
        "id": "qwxgf8z2mgd5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_collection.head()\n"
      ],
      "metadata": {
        "id": "m82KkFM9Lpyg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_collection['paper_text'] = df_collection['title'] + \". \" + df_collection['abstract']\n",
        "train_query_list = df_query_train['tweet_text']\n",
        "dev_query_list = df_query_dev['tweet_text']"
      ],
      "metadata": {
        "id": "YjcXTHNLRDm2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Encode Corpus and Queries with SentenceTransformer and Normalize Embeddings"
      ],
      "metadata": {
        "id": "KOoqLguda9Fw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "from torch.nn.functional import normalize\n",
        "\n",
        "bi_encoder = SentenceTransformer(\"multi-qa-MiniLM-L6-cos-v1\")\n",
        "\n",
        "\n",
        "corpus_embeddings = bi_encoder.encode(df_collection['paper_text'].tolist(), convert_to_tensor=True, show_progress_bar=True)\n",
        "\n",
        "train_query_embeddings = bi_encoder.encode(train_query_list, convert_to_tensor=True, show_progress_bar=True)\n",
        "dev_query_embeddings = bi_encoder.encode(dev_query_list, convert_to_tensor=True, show_progress_bar=True)\n",
        "from torch.nn.functional import normalize\n",
        "\n",
        "corpus_embeddings = normalize(corpus_embeddings, p=2, dim=1)\n",
        "train_query_embeddings = normalize(train_query_embeddings, p=2, dim=1)\n",
        "dev_query_embeddings = normalize(dev_query_embeddings, p=2, dim=1)\n",
        "\n"
      ],
      "metadata": {
        "id": "MLUzSuBvRHG9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ground_truth_train = dict(zip(df_query_train['post_id'], df_query_train['cord_uid']))\n",
        "query_ids_train = df_query_train['post_id'].tolist()\n",
        "\n",
        "ground_truth_dev = dict(zip(df_query_dev['post_id'], df_query_dev['cord_uid']))\n",
        "query_ids_dev = df_query_dev['post_id'].tolist()\n"
      ],
      "metadata": {
        "id": "hQsbPOGK1qTh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Cosine similartiy for query development"
      ],
      "metadata": {
        "id": "IGhXOCuIfn19"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.nn.functional import cosine_similarity\n",
        "\n",
        "top_k = 5\n",
        "top_k_results_train = []\n",
        "top_k_results_dev = []\n",
        "\n",
        "\n",
        "for query_vec in train_query_embeddings:\n",
        "    cos_scores = cosine_similarity(query_vec.unsqueeze(0), corpus_embeddings).squeeze(0)\n",
        "\n",
        "    if cos_scores.ndim == 0:\n",
        "        cos_scores = cos_scores.unsqueeze(0)\n",
        "\n",
        "    top_k_val = min(top_k, cos_scores.shape[0])\n",
        "    top_results = torch.topk(cos_scores, k=top_k_val)\n",
        "    top_k_results_train.append(top_results.indices.tolist())\n",
        "\n",
        "\n",
        "for query_vec in dev_query_embeddings:\n",
        "    cos_scores = cosine_similarity(query_vec.unsqueeze(0), corpus_embeddings).squeeze(0)\n",
        "\n",
        "    if cos_scores.ndim == 0:\n",
        "        cos_scores = cos_scores.unsqueeze(0)\n",
        "\n",
        "    top_k_val = min(top_k, cos_scores.shape[0])\n",
        "    top_results = torch.topk(cos_scores, k=top_k_val)\n",
        "    top_k_results_dev.append(top_results.indices.tolist())\n",
        "\n",
        "top_k_cord_uids_train = [\n",
        "    df_collection['cord_uid'].iloc[doc_indices].tolist() for doc_indices in top_k_results_train\n",
        "]\n",
        "\n",
        "top_k_cord_uids_dev = [\n",
        "    df_collection['cord_uid'].iloc[doc_indices].tolist() for doc_indices in top_k_results_dev\n",
        "]\n",
        "\n"
      ],
      "metadata": {
        "id": "pGb4kBmMbKow"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def get_cord_uids_from_indices(indices, df_collection):\n",
        "    return df_collection['cord_uid'].iloc[indices].tolist()\n",
        "\n",
        "\n",
        "top_k_results_train_with_ids = [\n",
        "    get_cord_uids_from_indices(doc_ids, df_collection) for doc_ids in top_k_results_train\n",
        "]\n",
        "\n",
        "top_k_results_dev_with_ids = [\n",
        "    get_cord_uids_from_indices(doc_ids, df_collection) for doc_ids in top_k_results_dev\n",
        "]\n",
        "\n",
        "\n",
        "train_actual_vs_predicted = [\n",
        "    {\n",
        "        'actual': ground_truth_train.get(query_ids_train[i]),\n",
        "        'predicted': top_k_results_train_with_ids[i]\n",
        "    }\n",
        "    for i in range(len(query_ids_train))\n",
        "]\n",
        "\n",
        "dev_actual_vs_predicted = [\n",
        "    {\n",
        "        'actual': ground_truth_dev.get(query_ids_dev[i]),\n",
        "        'predicted': top_k_results_dev_with_ids[i]\n",
        "    }\n",
        "    for i in range(len(query_ids_dev))\n",
        "]\n",
        "\n"
      ],
      "metadata": {
        "id": "DHCoo0_b3Jiv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_mrr(predictions, ground_truth_dict, query_ids):\n",
        "    total_score = 0.0\n",
        "\n",
        "\n",
        "    for i, query_id in enumerate(query_ids):\n",
        "        predicted_docs = predictions[i]\n",
        "        relevant_doc = ground_truth_dict.get(query_id)\n",
        "\n",
        "        if not relevant_doc:\n",
        "            continue\n",
        "\n",
        "        for rank, doc_id in enumerate(predicted_docs, start=1):\n",
        "            if doc_id == relevant_doc:\n",
        "                total_score += 1 / rank\n",
        "                break\n",
        "\n",
        "\n",
        "    return total_score / len(query_ids)\n",
        "\n",
        "def compute_recall_at_k(predictions, ground_truth_dict, query_ids):\n",
        "    total_hits = 0\n",
        "\n",
        "    for i, query_id in enumerate(query_ids):\n",
        "        predicted_docs = predictions[i]\n",
        "        relevant_doc = ground_truth_dict.get(query_id)\n",
        "\n",
        "        if not relevant_doc:\n",
        "            continue\n",
        "\n",
        "        if relevant_doc in predicted_docs:\n",
        "            total_hits += 1\n",
        "\n",
        "\n",
        "    recall = total_hits / len(query_ids)\n",
        "    return recall\n",
        "\n"
      ],
      "metadata": {
        "id": "WpsAevE6dvxF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluation the pretrained model"
      ],
      "metadata": {
        "id": "IOlmV-LpYOha"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for k in [1, 5, 10]:\n",
        "    trimmed_predictions_train = [doc_ids[:k] for doc_ids in top_k_results_train_with_ids]\n",
        "\n",
        "    mrr_score_train = compute_mrr(\n",
        "        predictions=trimmed_predictions_train,\n",
        "        ground_truth_dict=ground_truth_train,\n",
        "        query_ids=query_ids_train\n",
        "    )\n",
        "\n",
        "    print(f\"MRR@{k} for Train Set: {mrr_score_train:.4f}\")\n",
        "\n",
        "for k in [1, 5, 10]:\n",
        "    trimmed_predictions_dev = [doc_ids[:k] for doc_ids in top_k_results_dev_with_ids]\n",
        "\n",
        "    mrr_score_dev = compute_mrr(\n",
        "        predictions=trimmed_predictions_dev,\n",
        "        ground_truth_dict=ground_truth_dev,\n",
        "        query_ids=query_ids_dev\n",
        "    )\n",
        "\n",
        "    print(f\"MRR@{k} for Dev Set: {mrr_score_dev:.4f}\")\n",
        "\n",
        "for k in [1, 5, 10]:\n",
        "    trimmed_predictions_train = [doc_ids[:k] for doc_ids in top_k_results_train_with_ids]\n",
        "    recall_score_train = compute_recall_at_k(\n",
        "        predictions=trimmed_predictions_train,\n",
        "        ground_truth_dict=ground_truth_train,\n",
        "        query_ids=query_ids_train,\n",
        "    )\n",
        "    print(f\"Recall@{k} for Train Set: {recall_score_train:.4f}\")\n",
        "\n",
        "for k in [1, 5, 10]:\n",
        "    trimmed_predictions_dev = [doc_ids[:k] for doc_ids in top_k_results_dev_with_ids]\n",
        "    recall_score_dev = compute_recall_at_k(\n",
        "        predictions=trimmed_predictions_dev,\n",
        "        ground_truth_dict=ground_truth_dev,\n",
        "        query_ids=query_ids_dev,\n",
        "    )\n",
        "    print(f\"Recall@{k} for Dev Set: {recall_score_dev:.4f}\")\n"
      ],
      "metadata": {
        "id": "rF3z6Qd718s_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "| Metric       | Train Set | Dev Set  |\n",
        "|--------------|-----------|----------|\n",
        "| MRR@1        | 0.4312    | 0.4271   |\n",
        "| MRR@5        | 0.4943    | 0.4921   |\n",
        "| MRR@10       | 0.4943    | 0.4921   |\n",
        "| Recall@1     | 0.4312    | 0.4271   |\n",
        "| Recall@5     | 0.5959    | 0.6043   |\n",
        "| Recall@10    | 0.5959    | 0.6043   |\n"
      ],
      "metadata": {
        "id": "Rpus-5uzgeBh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Reranking Top-K Candidates with CrossEncoder\n"
      ],
      "metadata": {
        "id": "OE_9QFjrZayh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "def rerank_query(tweet_text, candidate_paper_texts):\n",
        "    pairs = [(tweet_text, doc_text) for doc_text in candidate_paper_texts]\n",
        "    scores = cross_encoder.predict(pairs)\n",
        "    sorted_indices = torch.argsort(torch.tensor(scores), descending=True)\n",
        "    return sorted_indices.tolist()\n",
        "\n",
        "reranked_top_k_results_train = []\n",
        "reranked_top_k_results_dev = []\n",
        "\n",
        "top_k = 5\n",
        "\n",
        "cord_uid_to_text = dict(zip(df_collection['cord_uid'], df_collection['paper_text']))\n",
        "\n",
        "# --- Rerank Train Queries ---\n",
        "for i, query_text in tqdm(enumerate(train_query_list), total=len(train_query_list), desc=\"Reranking Train Queries\"):\n",
        "    candidate_doc_ids = top_k_results_train[i]\n",
        "    candidate_texts = [cord_uid_to_text[df_collection['cord_uid'].iloc[idx]] for idx in candidate_doc_ids]\n",
        "    new_order = rerank_query(query_text, candidate_texts)\n",
        "    reranked_candidates = [candidate_doc_ids[idx] for idx in new_order]\n",
        "    reranked_top_k_results_train.append(reranked_candidates)\n",
        "\n",
        "# --- Rerank Dev Queries ---\n",
        "for i, query_text in tqdm(enumerate(dev_query_list), total=len(dev_query_list), desc=\"Reranking Dev Queries\"):\n",
        "    candidate_doc_ids = top_k_results_dev[i]\n",
        "    candidate_texts = [cord_uid_to_text[df_collection['cord_uid'].iloc[idx]] for idx in candidate_doc_ids]\n",
        "    new_order = rerank_query(query_text, candidate_texts)\n",
        "    reranked_candidates = [candidate_doc_ids[idx] for idx in new_order]\n",
        "    reranked_top_k_results_dev.append(reranked_candidates)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "i29-qrqZmipA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_cord_uids_from_indices(indices, df_collection):\n",
        "    return df_collection['cord_uid'].iloc[indices].tolist()\n",
        "\n",
        "reranked_top_k_results_train_with_ids = [\n",
        "    get_cord_uids_from_indices(doc_ids, df_collection) for doc_ids in reranked_top_k_results_train\n",
        "]\n",
        "\n",
        "reranked_top_k_results_dev_with_ids = [\n",
        "    get_cord_uids_from_indices(doc_ids, df_collection) for doc_ids in reranked_top_k_results_dev\n",
        "]\n"
      ],
      "metadata": {
        "id": "AzV_BGocbf26"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluate MRR and Recall@K after ReRanking on Train and Dev Sets\n"
      ],
      "metadata": {
        "id": "rM6J1i5DkB9o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for k in [1, 5, 10]:\n",
        "    trimmed_predictions_train = [doc_ids[:k] for doc_ids in reranked_top_k_results_train_with_ids]\n",
        "    trimmed_predictions_dev = [doc_ids[:k] for doc_ids in reranked_top_k_results_dev_with_ids]\n",
        "\n",
        "    mrr_score_train = compute_mrr(\n",
        "        predictions=trimmed_predictions_train,\n",
        "        ground_truth_dict=ground_truth_train,\n",
        "        query_ids=query_ids_train\n",
        "    )\n",
        "\n",
        "    mrr_score_dev = compute_mrr(\n",
        "        predictions=trimmed_predictions_dev,\n",
        "        ground_truth_dict=ground_truth_dev,\n",
        "        query_ids=query_ids_dev\n",
        "    )\n",
        "\n",
        "    print(f\"MRR@{k} for Train Set after ReRanking: {mrr_score_train:.4f}\")\n",
        "    print(f\"MRR@{k} for Dev Set after ReRanking: {mrr_score_dev:.4f}\")\n",
        "\n",
        "\n",
        "for k in [1, 5, 10]:\n",
        "    trimmed_predictions_train = [doc_ids[:k] for doc_ids in reranked_top_k_results_train_with_ids]\n",
        "    trimmed_predictions_dev = [doc_ids[:k] for doc_ids in reranked_top_k_results_dev_with_ids]\n",
        "\n",
        "    recall_score_train = compute_recall_at_k(\n",
        "        predictions=trimmed_predictions_train,\n",
        "        ground_truth_dict=ground_truth_train,\n",
        "        query_ids=query_ids_train\n",
        "    )\n",
        "\n",
        "    recall_score_dev = compute_recall_at_k(\n",
        "        predictions=trimmed_predictions_dev,\n",
        "        ground_truth_dict=ground_truth_dev,\n",
        "        query_ids=query_ids_dev\n",
        "    )\n",
        "\n",
        "    print(f\"Recall@{k} for Train Set after ReRanking: {recall_score_train:.4f}\")\n",
        "    print(f\"Recall@{k} for Dev Set after ReRanking: {recall_score_dev:.4f}\")\n",
        ""
      ],
      "metadata": {
        "id": "__arhjgubgAS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "| Metric      | k  | Train Set | Dev Set |\n",
        "|-------------|----|-----------|---------|\n",
        "| MRR         | 1  | 0.5014    | 0.5243  |\n",
        "| MRR         | 5  | 0.5400    | 0.5576  |\n",
        "| MRR         | 10 | 0.5400    | 0.5576  |\n",
        "| Recall      | 1  | 0.5014    | 0.5243  |\n",
        "| Recall      | 5  | 0.5959    | 0.6043  |\n",
        "| Recall      | 10 | 0.5959    | 0.6043  |\n"
      ],
      "metadata": {
        "id": "pY-4p6R5kolz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Creating Training Data with Positive and Negative Pairs\n"
      ],
      "metadata": {
        "id": "Ji8QmDRPkvwB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sentence_transformers import InputExample\n",
        "import random\n",
        "\n",
        "train_examples = []\n",
        "\n",
        "all_paper_texts = list(cord_uid_to_text.values())\n",
        "\n",
        "for idx, row in df_query_train.iterrows():\n",
        "    tweet_text = row['tweet_text']\n",
        "    correct_cord_uid = row['cord_uid']\n",
        "    correct_paper_text = cord_uid_to_text.get(correct_cord_uid, \"\")\n",
        "\n",
        "\n",
        "    train_examples.append(InputExample(texts=[tweet_text, correct_paper_text], label=1.0))\n",
        "\n",
        "\n",
        "    while True:\n",
        "        negative_paper_text = random.choice(all_paper_texts)\n",
        "        if negative_paper_text != correct_paper_text:\n",
        "            break\n",
        "\n",
        "    train_examples.append(InputExample(texts=[tweet_text, negative_paper_text], label=0.0))\n",
        "\n"
      ],
      "metadata": {
        "id": "FTtkD-pgkKUk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "batch_size = 64\n",
        "\n",
        "def simple_collate_fn(batch):\n",
        "    return batch\n",
        "\n",
        "train_dataloader = DataLoader(\n",
        "    train_examples,\n",
        "    shuffle=True,\n",
        "    batch_size=batch_size,\n",
        "    collate_fn=simple_collate_fn\n",
        ")\n",
        "\n",
        "\n",
        "model_name = \"cross-encoder/ms-marco-MiniLM-L-6-v2\"\n",
        "model = CrossEncoder(model_name, num_labels=1)"
      ],
      "metadata": {
        "id": "sjq_Q35pkxZM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Optimization and Fine-Tuning of CrossEncoder for Semantic Relevance Matching\n"
      ],
      "metadata": {
        "id": "ozq1UWSNnbPx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from torch import nn\n",
        "from torch.optim import AdamW\n",
        "\n",
        "\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model = model.to(device)\n",
        "\n",
        "learning_rate = 2e-5\n",
        "optimizer = AdamW(model.parameters(), lr=learning_rate)\n",
        "\n",
        "num_epochs = 3\n",
        "\n",
        "loss_fct = nn.BCEWithLogitsLoss()\n",
        "\n",
        "model.train()\n",
        "tokenizer = model.tokenizer\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    print(f\"\\nEpoch {epoch+1}/{num_epochs}\")\n",
        "    running_loss = 0.0\n",
        "\n",
        "    for batch in tqdm(train_dataloader, desc=f\"Training Epoch {epoch+1}\"):\n",
        "        texts = [example.texts for example in batch]\n",
        "        labels = torch.tensor([example.label for example in batch]).to(device)\n",
        "\n",
        "        texts_a = [text_pair[0] for text_pair in texts]\n",
        "        texts_b = [text_pair[1] for text_pair in texts]\n",
        "\n",
        "        inputs = tokenizer(\n",
        "            texts_a,\n",
        "            texts_b,\n",
        "            padding=True,\n",
        "            truncation=True,\n",
        "            return_tensors=\"pt\"\n",
        "        )\n",
        "        inputs = {k: v.to(device) for k, v in inputs.items()}\n",
        "\n",
        "\n",
        "        model.zero_grad()\n",
        "        scores = model(**inputs).logits.squeeze()\n",
        "\n",
        "\n",
        "        loss = loss_fct(scores, labels.float())\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "\n",
        "    avg_loss = running_loss / len(train_dataloader)\n",
        "    print(f\"Epoch {epoch+1} finished with avg loss: {avg_loss:.4f}\")\n",
        "\n",
        "output_model_path = \"./fine_tuned_cross_encoder\"\n",
        "model.save(output_model_path)\n",
        "\n",
        "print(f\"\\nModel fine-tuned and saved at: {output_model_path}\")\n",
        "\n"
      ],
      "metadata": {
        "id": "zf_rkRLzrj1p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for k in [1, 5, 10]:\n",
        "\n",
        "    trimmed_predictions_train = [doc_ids[:k] for doc_ids in reranked_top_k_results_train_with_ids]\n",
        "    trimmed_predictions_dev = [doc_ids[:k] for doc_ids in reranked_top_k_results_dev_with_ids]\n",
        "\n",
        "\n",
        "    mrr_score_train = compute_mrr(\n",
        "        predictions=trimmed_predictions_train,\n",
        "        ground_truth_dict=ground_truth_train,\n",
        "        query_ids=query_ids_train\n",
        "    )\n",
        "\n",
        "    mrr_score_dev = compute_mrr(\n",
        "        predictions=trimmed_predictions_dev,\n",
        "        ground_truth_dict=ground_truth_dev,\n",
        "        query_ids=query_ids_dev\n",
        "    )\n",
        "\n",
        "    recall_score_train = compute_recall_at_k(\n",
        "        predictions=trimmed_predictions_train,\n",
        "        ground_truth_dict=ground_truth_train,\n",
        "        query_ids=query_ids_train\n",
        "    )\n",
        "\n",
        "    recall_score_dev = compute_recall_at_k(\n",
        "        predictions=trimmed_predictions_dev,\n",
        "        ground_truth_dict=ground_truth_dev,\n",
        "        query_ids=query_ids_dev\n",
        "    )\n",
        "\n",
        "\n",
        "    print(f\"MRR@{k} for Train after Re-ranking: {mrr_score_train:.4f}\")\n",
        "    print(f\"MRR@{k} for Dev after Re-ranking: {mrr_score_dev:.4f}\")\n",
        "\n",
        "\n",
        "    print(f\"Recall@{k} for Train after Re-ranking: {recall_score_train:.4f}\")\n",
        "    print(f\"Recall@{k} for Dev after Re-ranking: {recall_score_dev:.4f}\")\n"
      ],
      "metadata": {
        "id": "1Uh8HUfw0SIB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## MRR and Recall after Re-Ranking with 1 epoch\n",
        "\n",
        "| Metric    | Train Set | Dev Set |\n",
        "|-----------|-----------|---------|\n",
        "| MRR@1     | 0.5359    | 0.5493  |\n",
        "| Recall@1  | 0.5359    | 0.5493  |\n",
        "| MRR@5     | 0.5623    | 0.5706  |\n",
        "| Recall@5  | 0.5959    | 0.6043  |\n",
        "| MRR@10    | 0.5623    | 0.5706  |\n",
        "| Recall@10 | 0.5959    | 0.6043  |\n"
      ],
      "metadata": {
        "id": "JW27wFy2uQ6W"
      }
    }
  ]
}