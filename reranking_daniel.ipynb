{
 "cells": [
  {
   "cell_type": "code",
   "metadata": {
    "executionInfo": {
     "elapsed": 403,
     "status": "ok",
     "timestamp": 1742975967136,
     "user": {
      "displayName": "Yavuz",
      "userId": "01318046262282431930"
     },
     "user_tz": -60
    },
    "id": "rQPqDKP_QHFM",
    "ExecuteTime": {
     "end_time": "2025-05-12T18:11:12.508840Z",
     "start_time": "2025-05-12T18:11:08.104369Z"
    }
   },
   "source": [
    "import torch\n",
    "from torch.nn.functional import cosine_similarity\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "import json\n",
    "from pathlib import Path\n",
    "import re\n",
    "import random\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandarallel import pandarallel\n",
    "\n",
    "pandarallel.initialize(progress_bar=True)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Pandarallel will run on 8 workers.\n",
      "INFO: Pandarallel will use standard multiprocessing data transfer (pipe) to transfer data between the main process and workers.\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vpDCfBMouNAL"
   },
   "source": [
    "# 1) Importing query and collection data"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1742975971100,
     "user": {
      "displayName": "Yavuz",
      "userId": "01318046262282431930"
     },
     "user_tz": -60
    },
    "id": "2GQI4HcKR6hS",
    "ExecuteTime": {
     "end_time": "2025-05-12T18:11:12.545789Z",
     "start_time": "2025-05-12T18:11:12.521388Z"
    }
   },
   "source": [
    "PATH_COLLECTION_DATA = 'subtask_4b/subtask4b_collection_data.pkl' \n",
    "df_collection = pd.read_pickle(PATH_COLLECTION_DATA)"
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "metadata": {
    "executionInfo": {
     "elapsed": 17,
     "status": "ok",
     "timestamp": 1742976006985,
     "user": {
      "displayName": "Yavuz",
      "userId": "01318046262282431930"
     },
     "user_tz": -60
    },
    "id": "VqxjYq2tYDmE",
    "ExecuteTime": {
     "end_time": "2025-05-12T18:11:12.708197Z",
     "start_time": "2025-05-12T18:11:12.660480Z"
    }
   },
   "source": [
    "PATH_QUERY_TRAIN_DATA = 'subtask_4b/subtask4b_query_tweets_train.tsv'\n",
    "PATH_QUERY_DEV_DATA = 'subtask_4b/subtask4b_query_tweets_dev.tsv' \n",
    "df_query_train = pd.read_csv(PATH_QUERY_TRAIN_DATA, sep = '\\t')\n",
    "df_query_dev = pd.read_csv(PATH_QUERY_DEV_DATA, sep = '\\t')"
   ],
   "outputs": [],
   "execution_count": 3
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jr_BDzufPmmP"
   },
   "source": [
    "# 2) Running the BM25 baseline\n",
    "The following code runs a BM25 baseline.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3156,
     "status": "ok",
     "timestamp": 1742976045832,
     "user": {
      "displayName": "Yavuz",
      "userId": "01318046262282431930"
     },
     "user_tz": -60
    },
    "id": "BHfJ7ItxO8u8",
    "outputId": "8a4d8f08-31c0-4a9d-814e-b921b80fbe35",
    "ExecuteTime": {
     "end_time": "2025-05-12T18:11:12.825685Z",
     "start_time": "2025-05-12T18:11:12.821948Z"
    }
   },
   "source": [
    "from rank_bm25 import BM25Okapi"
   ],
   "outputs": [],
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "metadata": {
    "executionInfo": {
     "elapsed": 1414,
     "status": "ok",
     "timestamp": 1742976047296,
     "user": {
      "displayName": "Yavuz",
      "userId": "01318046262282431930"
     },
     "user_tz": -60
    },
    "id": "jXCC7K_ZPQL2",
    "ExecuteTime": {
     "end_time": "2025-05-12T18:11:13.444750Z",
     "start_time": "2025-05-12T18:11:12.910333Z"
    }
   },
   "source": [
    "# Create the BM25 corpus\n",
    "corpus = df_collection[:][['title', 'abstract']].apply(lambda x: f\"{x['title']} {x['abstract']}\", axis=1).tolist()\n",
    "cord_uids = df_collection[:]['cord_uid'].tolist()\n",
    "tokenized_corpus = [doc.split(' ') for doc in corpus]\n",
    "bm25 = BM25Okapi(tokenized_corpus)"
   ],
   "outputs": [],
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "metadata": {
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1742976047304,
     "user": {
      "displayName": "Yavuz",
      "userId": "01318046262282431930"
     },
     "user_tz": -60
    },
    "id": "e8NeJWGYPQZG",
    "ExecuteTime": {
     "end_time": "2025-05-12T18:11:13.458371Z",
     "start_time": "2025-05-12T18:11:13.456101Z"
    }
   },
   "source": [
    "def get_top_cord_uids(query):\n",
    "  text2bm25top = {}\n",
    "  if query in text2bm25top.keys():\n",
    "      return text2bm25top[query]\n",
    "  else:\n",
    "      tokenized_query = query.split(' ')\n",
    "      doc_scores = bm25.get_scores(tokenized_query)\n",
    "      indices = np.argsort(-doc_scores)[:100] # @k: how many docs shall the ranked list include?\n",
    "      bm25_topk = [cord_uids[x] for x in indices]\n",
    "\n",
    "      text2bm25top[query] = bm25_topk\n",
    "      return bm25_topk\n"
   ],
   "outputs": [],
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-12T18:11:13.555099Z",
     "start_time": "2025-05-12T18:11:13.502324Z"
    }
   },
   "source": [
    "# Retrieve top100 candidates using the BM25 model\n",
    "\n",
    "train_pkl_path = 'df_query_train_top100.pkl'\n",
    "dev_pkl_path = 'df_query_dev_top100.pkl'\n",
    "\n",
    "if not os.path.exists(train_pkl_path):\n",
    "    df_query_train['bm25_topk'] = df_query_train['tweet_text'].parallel_apply(lambda x: get_top_cord_uids(x))\n",
    "    df_query_train.to_pickle(train_pkl_path)\n",
    "else:\n",
    "    df_query_train = pd.read_pickle(train_pkl_path)\n",
    "\n",
    "if not os.path.exists(dev_pkl_path):\n",
    "    df_query_dev['bm25_topk'] = df_query_dev['tweet_text'].parallel_apply(lambda x: get_top_cord_uids(x))\n",
    "    df_query_dev.to_pickle(dev_pkl_path)\n",
    "else:\n",
    "    df_query_dev = pd.read_pickle(dev_pkl_path)"
   ],
   "outputs": [],
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-12T18:11:13.571840Z",
     "start_time": "2025-05-12T18:11:13.563737Z"
    }
   },
   "source": [
    "df_query_train.head()"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "   post_id                                         tweet_text  cord_uid  \\\n",
       "0        0  Oral care in rehabilitation medicine: oral vul...  htlvpvz5   \n",
       "1        1  this study isn't receiving sufficient attentio...  4kfl29ul   \n",
       "2        2  thanks, xi jinping. a reminder that this study...  jtwb17u8   \n",
       "3        3  Taiwan - a population of 23 million has had ju...  0w9k8iy1   \n",
       "4        4  Obtaining a diagnosis of autism in lower incom...  tiqksd69   \n",
       "\n",
       "                               normalized_tweet_text  \\\n",
       "0  oral care in rehabilitation medicine: oral vul...   \n",
       "1  this study isn't receiving sufficient attentio...   \n",
       "2  thanks, xi jinping. a reminder that this study...   \n",
       "3  taiwan - a population of 23 million has had ju...   \n",
       "4  obtaining a diagnosis of autism in lower incom...   \n",
       "\n",
       "                                  cleaned_tweet_text  \\\n",
       "0  oral care rehabilit medicin oral vulner oral m...   \n",
       "1  studi isnt receiv suffici attent reveal blackl...   \n",
       "2  thank xi jinp remind studi conclud nonpharmace...   \n",
       "3  taiwan popul 23 million 600 case 7 death wides...   \n",
       "4  obtain diagnosi autism lower incom countri tak...   \n",
       "\n",
       "                                         final_query  \\\n",
       "0  oral care rehabilit medicin oral vulner oral m...   \n",
       "1  studi isnt receiv suffici attent reveal blackl...   \n",
       "2  thank xi jinp remind studi conclud nonpharmace...   \n",
       "3  taiwan popul 23 million 600 case 7 death wides...   \n",
       "4  obtain diagnosi autism lower incom countri tak...   \n",
       "\n",
       "                                           bm25_topk  in_topx  \n",
       "0  [htlvpvz5, h7hj64q5, trmwm9qq, 65gedo6u, rwgqk...      1.0  \n",
       "1  [apqzyln2, asdcpvhx, 33znyrn8, ljcdfmbu, 296il...      0.0  \n",
       "2  [jtwb17u8, veeavho5, mwj0xc3q, 8hkxbxz9, a0q61...      1.0  \n",
       "3  [lsgm7y5t, l5ogbl5p, l4y7v729, x14iywtr, 0w9k8...      0.2  \n",
       "4  [tiqksd69, b0dzhsrh, k7smwz6w, aqbhxv1f, 0u330...      1.0  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>post_id</th>\n",
       "      <th>tweet_text</th>\n",
       "      <th>cord_uid</th>\n",
       "      <th>normalized_tweet_text</th>\n",
       "      <th>cleaned_tweet_text</th>\n",
       "      <th>final_query</th>\n",
       "      <th>bm25_topk</th>\n",
       "      <th>in_topx</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Oral care in rehabilitation medicine: oral vul...</td>\n",
       "      <td>htlvpvz5</td>\n",
       "      <td>oral care in rehabilitation medicine: oral vul...</td>\n",
       "      <td>oral care rehabilit medicin oral vulner oral m...</td>\n",
       "      <td>oral care rehabilit medicin oral vulner oral m...</td>\n",
       "      <td>[htlvpvz5, h7hj64q5, trmwm9qq, 65gedo6u, rwgqk...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>this study isn't receiving sufficient attentio...</td>\n",
       "      <td>4kfl29ul</td>\n",
       "      <td>this study isn't receiving sufficient attentio...</td>\n",
       "      <td>studi isnt receiv suffici attent reveal blackl...</td>\n",
       "      <td>studi isnt receiv suffici attent reveal blackl...</td>\n",
       "      <td>[apqzyln2, asdcpvhx, 33znyrn8, ljcdfmbu, 296il...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>thanks, xi jinping. a reminder that this study...</td>\n",
       "      <td>jtwb17u8</td>\n",
       "      <td>thanks, xi jinping. a reminder that this study...</td>\n",
       "      <td>thank xi jinp remind studi conclud nonpharmace...</td>\n",
       "      <td>thank xi jinp remind studi conclud nonpharmace...</td>\n",
       "      <td>[jtwb17u8, veeavho5, mwj0xc3q, 8hkxbxz9, a0q61...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Taiwan - a population of 23 million has had ju...</td>\n",
       "      <td>0w9k8iy1</td>\n",
       "      <td>taiwan - a population of 23 million has had ju...</td>\n",
       "      <td>taiwan popul 23 million 600 case 7 death wides...</td>\n",
       "      <td>taiwan popul 23 million 600 case 7 death wides...</td>\n",
       "      <td>[lsgm7y5t, l5ogbl5p, l4y7v729, x14iywtr, 0w9k8...</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Obtaining a diagnosis of autism in lower incom...</td>\n",
       "      <td>tiqksd69</td>\n",
       "      <td>obtaining a diagnosis of autism in lower incom...</td>\n",
       "      <td>obtain diagnosi autism lower incom countri tak...</td>\n",
       "      <td>obtain diagnosi autism lower incom countri tak...</td>\n",
       "      <td>[tiqksd69, b0dzhsrh, k7smwz6w, aqbhxv1f, 0u330...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 8
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-12T18:11:13.624670Z",
     "start_time": "2025-05-12T18:11:13.622330Z"
    }
   },
   "source": [
    "# Evaluate retrieved candidates using MRR@k\n",
    "def get_performance_mrr(data, col_gold, col_pred, list_k = [1, 5, 10]):\n",
    "    d_performance = {}\n",
    "    for k in list_k:\n",
    "        data[\"in_topx\"] = data.apply(lambda x: (1/([i for i in x[col_pred][:k]].index(x[col_gold]) + 1) if x[col_gold] in [i for i in x[col_pred][:k]] else 0), axis=1)\n",
    "        d_performance[k] = data[\"in_topx\"].mean()\n",
    "    return d_performance"
   ],
   "outputs": [],
   "execution_count": 9
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-12T18:11:13.980203Z",
     "start_time": "2025-05-12T18:11:13.771031Z"
    }
   },
   "source": [
    "# Evaluate retrieved candidates using MRR@k\n",
    "results_train = get_performance_mrr(df_query_train, 'cord_uid', 'bm25_topk')\n",
    "results_dev = get_performance_mrr(df_query_dev, 'cord_uid', 'bm25_topk')\n",
    "\n",
    "# Printed MRR@k results\n",
    "print(f\"Results on the train set: {dict((k, float(v)) for k, v in results_train.items())}\")\n",
    "print(f\"Results on the dev set: {dict((k, float(v)) for k, v in results_dev.items())}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results on the train set: {1: 0.5731735781529604, 5: 0.625250914183459, 10: 0.6308237901348459}\n",
      "Results on the dev set: {1: 0.5657142857142857, 5: 0.616095238095238, 10: 0.6224325396825396}\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-12T18:11:14.005427Z",
     "start_time": "2025-05-12T18:11:13.999377Z"
    }
   },
   "source": [
    "df_query_dev.head()"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "   post_id                                         tweet_text  cord_uid  \\\n",
       "0       16  covid recovery: this study from the usa reveal...  3qvh482o   \n",
       "1       69  \"Among 139 clients exposed to two symptomatic ...  r58aohnu   \n",
       "2       73  I recall early on reading that researchers who...  sts48u9i   \n",
       "3       93  You know you're credible when NIH website has ...  3sr2exq9   \n",
       "4       96  Resistance to antifungal medications is a grow...  ybwwmyqy   \n",
       "\n",
       "                               normalized_tweet_text  \\\n",
       "0  covid recovery: this study from the usa reveal...   \n",
       "1  \"among 139 clients exposed to two symptomatic ...   \n",
       "2  i recall early on reading that researchers wor...   \n",
       "3  you know you're credible when national institu...   \n",
       "4  resistance to antifungal medications is a grow...   \n",
       "\n",
       "                                  cleaned_tweet_text  \\\n",
       "0  covid recoveri studi usa reveal proport case e...   \n",
       "1  among 139 client expos two symptomat hair styl...   \n",
       "2  recal earli read research who examin coronavir...   \n",
       "3  know your credibl nih websit paper ðŸ’ƒðŸ’ƒ someon p...   \n",
       "4  resist antifung medic grow issu global scope d...   \n",
       "\n",
       "                                         final_query  \\\n",
       "0  covid recoveri studi usa reveal proport case e...   \n",
       "1  among 139 client expos two symptomat hair styl...   \n",
       "2  recal earli read research who examin coronavir...   \n",
       "3  know your credibl nih websit paper ðŸ’ƒðŸ’ƒ someon p...   \n",
       "4  resist antifung medic grow issu global scope d...   \n",
       "\n",
       "                                           bm25_topk  in_topx  \n",
       "0  [25aj8rj5, 66g5lpm6, o4vvlmr4, vmmwtdia, trrg1...      0.0  \n",
       "1  [r58aohnu, p0kg6dyz, s2vckt2w, yrowv62k, g5hg3...      1.0  \n",
       "2  [mkwgkkoi, gruir7aw, xavegbty, vx1hjh26, ntxuf...      0.0  \n",
       "3  [3sr2exq9, sv48gjkk, tx8ypqsm, z795y51f, k0f4c...      1.0  \n",
       "4  [ybwwmyqy, ouvq2wpq, rs3umc1x, sxx3yid9, vabb2...      1.0  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>post_id</th>\n",
       "      <th>tweet_text</th>\n",
       "      <th>cord_uid</th>\n",
       "      <th>normalized_tweet_text</th>\n",
       "      <th>cleaned_tweet_text</th>\n",
       "      <th>final_query</th>\n",
       "      <th>bm25_topk</th>\n",
       "      <th>in_topx</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>16</td>\n",
       "      <td>covid recovery: this study from the usa reveal...</td>\n",
       "      <td>3qvh482o</td>\n",
       "      <td>covid recovery: this study from the usa reveal...</td>\n",
       "      <td>covid recoveri studi usa reveal proport case e...</td>\n",
       "      <td>covid recoveri studi usa reveal proport case e...</td>\n",
       "      <td>[25aj8rj5, 66g5lpm6, o4vvlmr4, vmmwtdia, trrg1...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>69</td>\n",
       "      <td>\"Among 139 clients exposed to two symptomatic ...</td>\n",
       "      <td>r58aohnu</td>\n",
       "      <td>\"among 139 clients exposed to two symptomatic ...</td>\n",
       "      <td>among 139 client expos two symptomat hair styl...</td>\n",
       "      <td>among 139 client expos two symptomat hair styl...</td>\n",
       "      <td>[r58aohnu, p0kg6dyz, s2vckt2w, yrowv62k, g5hg3...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>73</td>\n",
       "      <td>I recall early on reading that researchers who...</td>\n",
       "      <td>sts48u9i</td>\n",
       "      <td>i recall early on reading that researchers wor...</td>\n",
       "      <td>recal earli read research who examin coronavir...</td>\n",
       "      <td>recal earli read research who examin coronavir...</td>\n",
       "      <td>[mkwgkkoi, gruir7aw, xavegbty, vx1hjh26, ntxuf...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>93</td>\n",
       "      <td>You know you're credible when NIH website has ...</td>\n",
       "      <td>3sr2exq9</td>\n",
       "      <td>you know you're credible when national institu...</td>\n",
       "      <td>know your credibl nih websit paper ðŸ’ƒðŸ’ƒ someon p...</td>\n",
       "      <td>know your credibl nih websit paper ðŸ’ƒðŸ’ƒ someon p...</td>\n",
       "      <td>[3sr2exq9, sv48gjkk, tx8ypqsm, z795y51f, k0f4c...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>96</td>\n",
       "      <td>Resistance to antifungal medications is a grow...</td>\n",
       "      <td>ybwwmyqy</td>\n",
       "      <td>resistance to antifungal medications is a grow...</td>\n",
       "      <td>resist antifung medic grow issu global scope d...</td>\n",
       "      <td>resist antifung medic grow issu global scope d...</td>\n",
       "      <td>[ybwwmyqy, ouvq2wpq, rs3umc1x, sxx3yid9, vabb2...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 11
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3) BERT Embeddings pre-computation"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-12T18:11:14.142Z",
     "start_time": "2025-05-12T18:11:14.092480Z"
    }
   },
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "def get_device():\n",
    "    if torch.cuda.is_available():\n",
    "        return \"cuda\"\n",
    "    elif torch.backends.mps.is_available():\n",
    "        return \"mps\"\n",
    "    else:\n",
    "        return \"cpu\"\n",
    "\n",
    "DEVICE = get_device()\n",
    "print(f\"Gonna run pytorch on {DEVICE}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gonna run pytorch on mps\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-12T18:11:14.246849Z",
     "start_time": "2025-05-12T18:11:14.240599Z"
    }
   },
   "source": [
    "# get token embeddings of a specified text passage from some model\n",
    "def get_token_embeddings(text, tokenizer, model, device):\n",
    "    inputs = tokenizer(text, return_tensors='pt', truncation=True, max_length=512)\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "\n",
    "    outputs = model(**inputs)\n",
    "    token_embeddings = outputs.last_hidden_state.squeeze(0)\n",
    "    attention_mask = inputs['attention_mask'].squeeze(0).bool()\n",
    "    token_embeddings = token_embeddings[attention_mask] \n",
    "    return token_embeddings\n",
    "\n",
    "# pre compute all the token embeddings of the documents\n",
    "def build_and_save_doc_embeddings(\n",
    "    docs_df,\n",
    "    model_name,\n",
    "    save_dir,\n",
    "    device,\n",
    "    max_len=512,\n",
    "    batch_size=16\n",
    "):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    model = AutoModel.from_pretrained(model_name).to(device)\n",
    "    model.eval()\n",
    "    \n",
    "    save_path = Path(\"doc_embeddings_\" + save_dir)\n",
    "    save_path.mkdir(parents=True, exist_ok=True)\n",
    "    metadata_path = save_path / \"metadata.json\"\n",
    "    \n",
    "    if metadata_path.exists():\n",
    "        with open(metadata_path, \"r\") as f:\n",
    "            metadata = json.load(f)\n",
    "    else:\n",
    "        metadata = {}\n",
    "    \n",
    "    print(\"Precomputing document embeddings.\")\n",
    "    \n",
    "    texts = []\n",
    "    doc_ids = []\n",
    "    indices = []\n",
    "    for i, row in tqdm(docs_df.iterrows(), total=len(docs_df)):\n",
    "        doc_id = row.get(\"cord_uid\", f\"doc_{i}\")\n",
    "        text = str(row.get('title', '')) + \" \" + str(row.get('abstract', '')) + \" Authors: \" + str(row.get('authors', ''))\n",
    "        texts.append(text)\n",
    "        doc_ids.append(doc_id)\n",
    "        indices.append(i)\n",
    "    \n",
    "    for start_idx in tqdm(range(0, len(texts), batch_size)):\n",
    "        end_idx = min(start_idx + batch_size, len(texts))\n",
    "        batch_texts = texts[start_idx:end_idx]\n",
    "        batch_doc_ids = doc_ids[start_idx:end_idx]\n",
    "        batch_indices = indices[start_idx:end_idx]\n",
    "        \n",
    "        inputs = tokenizer(batch_texts, return_tensors='pt', padding=True, truncation=True, max_length=max_len).to(device)\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "        q_emb_batch = outputs.last_hidden_state  # [batch_size, L, D]\n",
    "        attention_mask = inputs['attention_mask'].to(torch.bool)\n",
    "        \n",
    "        for i in range(len(batch_texts)):\n",
    "            att_mask = attention_mask[i]\n",
    "            embeddings = q_emb_batch[i][att_mask]\n",
    "            doc_id = batch_doc_ids[i]\n",
    "\n",
    "            file_path = Path(f\"doc_embeddings_{save_dir}\") / f\"{doc_id}.pt\"\n",
    "            torch.save(embeddings, file_path)\n",
    "            n_tokens = embeddings.shape[0]\n",
    "\n",
    "            if doc_id not in metadata:\n",
    "                metadata[doc_id] = {\n",
    "                    \"length\": min(n_tokens, max_len),\n",
    "                    \"path\": str(file_path)\n",
    "                }\n",
    "\n",
    "    metadata_path = Path(f\"doc_embeddings_{save_dir}\") / \"metadata.json\"\n",
    "    with open(metadata_path, \"w\") as f:\n",
    "        json.dump(metadata, f)\n",
    "    \n",
    "    return metadata\n",
    "\n",
    "# either precompute or load precomputed doc embeddings\n",
    "def get_precomputed_doc_embeddings(save_name):\n",
    "    def split_at_slash(s):\n",
    "        if '/' in s:\n",
    "            return s.split('/', 1)\n",
    "        else:\n",
    "            return ['', s]\n",
    "        \n",
    "    if not os.path.exists(\"doc_embeddings_\" + split_at_slash(save_name)[1] + \"/metadata.json\"):\n",
    "        metadata = build_and_save_doc_embeddings(df_collection, \"allenai/scibert_scivocab_uncased\", save_name, DEVICE)\n",
    "    else:\n",
    "        with open(\"doc_embeddings_\" + save_name + \"/metadata.json\", \"r\") as f:\n",
    "            metadata = json.load(f)\n",
    "    return metadata"
   ],
   "outputs": [],
   "execution_count": 13
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-12T18:18:42.988775Z",
     "start_time": "2025-05-12T18:11:14.326231Z"
    }
   },
   "source": [
    "doc_embeddings_allenai = get_precomputed_doc_embeddings(\"allenai\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precomputing document embeddings.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "  0%|          | 0/7718 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "78c4a1451d3b4649925db913953007ac"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "  0%|          | 0/483 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "4a5a89fa29194996b07300bb7deb21f2"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 14
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-12T18:18:43.283390Z",
     "start_time": "2025-05-12T18:18:43.267832Z"
    }
   },
   "source": [
    "# doc_embeddings_allenai = {\n",
    "#     k: {'length': v['length'], 'path': v['path'].replace('all_embeddings', 'allenai')}\n",
    "#     for k, v in doc_embeddings_allenai.items()\n",
    "# }"
   ],
   "outputs": [],
   "execution_count": 15
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 4) Neural Re-Ranking"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Let's create interface for generating convolutional layers for n-grams, calculate the similarity match matrix, generating kernel pooling object and the reranking function itself"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NGram Convolutional Layers class and Similarity Matrix computation:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-12T18:22:39.361116Z",
     "start_time": "2025-05-12T18:22:39.340438Z"
    }
   },
   "source": [
    "class NGramConvLayer(nn.Module):\n",
    "    def __init__(self, in_channels=1, out_channels=8, kernel_size=3):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Conv1d(in_channels, out_channels, kernel_size, padding=kernel_size//2)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # x shape: [batch_size, 1, seq_len_q, seq_len_d]\n",
    "        return F.relu(self.conv(x))\n",
    "\n",
    "def compute_similarity_matrix(q_embs, d_embs):\n",
    "    q_norm = F.normalize(q_embs, p=2, dim=1)\n",
    "    d_norm = F.normalize(d_embs, p=2, dim=1)\n",
    "    # cosine similarity matrix\n",
    "    return torch.mm(q_norm, d_norm.T)\n",
    "\n",
    "def process_query_doc(q_embs, d_embs, conv_layer=None):\n",
    "    \"\"\"\n",
    "    q_embs: [L_q, D]\n",
    "    d_embs: [L_d, D]\n",
    "    conv_layer: nn.Module, optional - for modeling n-grams.\n",
    "    \"\"\"\n",
    "    sim_matrix = compute_similarity_matrix(q_embs, d_embs)  # [L_q, L_d]\n",
    "    if conv_layer:\n",
    "        input_tensor = sim_matrix.unsqueeze(0).unsqueeze(0)  # shape: [1,1,L_q,L_d]\n",
    "        conv_output = conv_layer(input_tensor)  # shape: [1,out_channels,L_q,L_d]\n",
    "        pooled = conv_output.max(dim=2)[0].max(dim=2)[0]  # [out_channels]\n",
    "        features = pooled\n",
    "    else:\n",
    "        features = sim_matrix.flatten()\n",
    "    return features\n",
    "\n",
    "def create_ngram_conv_layer(ngram_size):\n",
    "    return NGramConvLayer(in_channels=1, out_channels=8, kernel_size=ngram_size)"
   ],
   "outputs": [],
   "execution_count": 21
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kernel Pooling Definition"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-12T18:22:41.129790Z",
     "start_time": "2025-05-12T18:22:41.120525Z"
    }
   },
   "source": [
    "class KernelPooling(nn.Module):\n",
    "    def __init__(self, mus, sigmas):\n",
    "        super().__init__()\n",
    "        self.mus = torch.tensor(mus).view(1, -1)  # shape: [1, num_kernels]\n",
    "        self.sigmas = torch.tensor(sigmas).view(1, -1)\n",
    "\n",
    "    def forward(self, sim_matrix):\n",
    "        # sim_matrix: [L_q, L_d]\n",
    "        sim_matrix = sim_matrix.unsqueeze(0).unsqueeze(0)  # [1,1,L_q,L_d]\n",
    "\n",
    "        mus = self.mus.to(sim_matrix.device)\n",
    "        sigmas = self.sigmas.to(sim_matrix.device)\n",
    "\n",
    "        kernel_vals = torch.exp(- (sim_matrix - mus.reshape(1, -1, 1, 1))**2 / (2 * sigmas.reshape(1, -1, 1, 1)**2))\n",
    "        pooled = kernel_vals.sum(dim=3).sum(dim=2)  # shape: [1, num_kernels]\n",
    "        return pooled.squeeze(0)  # shape: [num_kernels]"
   ],
   "outputs": [],
   "execution_count": 22
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reranking"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-12T18:48:47.520552Z",
     "start_time": "2025-05-12T18:48:47.507585Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class EmbeddingsLoader():\n",
    "    doc_embeddings = {}\n",
    "\n",
    "    @classmethod\n",
    "    def get_embeddings(cls):\n",
    "        if cls.doc_embeddings:\n",
    "            return cls.doc_embeddings\n",
    "        else:\n",
    "            for doc_id, data in tqdm(doc_embeddings_allenai.items(), desc=\"loading_doc_embeddings\"):\n",
    "                emb = torch.load(data[\"path\"], map_location=DEVICE)\n",
    "                cls.doc_embeddings[doc_id] = emb\n",
    "            return cls.doc_embeddings"
   ],
   "outputs": [],
   "execution_count": 35
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-12T19:08:43.621978Z",
     "start_time": "2025-05-12T19:08:43.591190Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def rerank(df, save_name, device, mus=None, sigmas=None,\n",
    "           ngram_size=1,  # 1 means no convolution, higher for n-grams\n",
    "           conv_channels=8):  # number of convolution filters\n",
    "    df[f'{save_name}_scores'] = [[]] * len(df)\n",
    "    if mus is None:\n",
    "        mus = [-1.0, -0.5, 0.0, 0.5, 1.0]\n",
    "    if sigmas is None:\n",
    "        sigmas = [0.1] * len(mus)\n",
    "\n",
    "\n",
    "    knrm_model = KNRM(mus, sigmas).to(device)\n",
    "    knrm_model.load_state_dict(torch.load(model_path))\n",
    "    knrm_model.eval()\n",
    "\n",
    "    model_name = \"allenai/scibert_scivocab_uncased\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    model = AutoModel.from_pretrained(model_name).to(device)\n",
    "\n",
    "    kernel_pool = KernelPooling(mus, sigmas).to(device)\n",
    "    conv_layer = None\n",
    "    if ngram_size > 1:\n",
    "        conv_layer = NGramConvLayer(in_channels=1, out_channels=conv_channels, kernel_size=ngram_size).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for idx, row in tqdm(df.iterrows(), total=len(df), desc='calculating_scores_for_reranking'):\n",
    "            tweet_text = row['tweet_text']\n",
    "            pre_ranked_docs = row['bm25_topk']\n",
    "            q_emb = get_token_embeddings(tweet_text, tokenizer, model, device=device)\n",
    "            q_emb = q_emb.to(device)\n",
    "            q_norm = q_emb / q_emb.norm(dim=1, keepdim=True)\n",
    "\n",
    "            scores = []\n",
    "            for doc in pre_ranked_docs:\n",
    "                emb = EmbeddingsLoader().get_embeddings()[doc]\n",
    "                length = doc_embeddings_allenai[doc][\"length\"]\n",
    "                d_emb = emb[:length]\n",
    "                d_emb = d_emb.to(device)\n",
    "                d_norm = d_emb / d_emb.norm(dim=1, keepdim=True)\n",
    "\n",
    "                if conv_layer:\n",
    "                    features = process_query_doc(q_norm, d_norm, conv_layer)\n",
    "                    score = features.sum().item()\n",
    "                else:\n",
    "                    sim_matrix = torch.mm(q_norm, d_norm.T)\n",
    "                    pooled_features = kernel_pool(sim_matrix)\n",
    "                    score = pooled_features.sum().item()\n",
    "                scores.append(score)\n",
    "\n",
    "            df.at[idx, f'{save_name}_scores'] = scores\n",
    "\n",
    "        def sort_docs_by_score(row):\n",
    "            doc_ids = row['bm25_topk']\n",
    "            scores = row[f'{save_name}_scores']\n",
    "            sorted_docs = [doc for doc, _ in sorted(zip(doc_ids, scores), key=lambda x: x[1], reverse=True)]\n",
    "            return sorted_docs\n",
    "\n",
    "        df[f'{save_name}_topk'] = df.parallel_apply(sort_docs_by_score, axis=1)\n",
    "\n",
    "    return df"
   ],
   "outputs": [],
   "execution_count": 39
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 4.1) KNRM with plain BERT"
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-12T18:43:45.777231Z",
     "start_time": "2025-05-12T18:43:43.426227Z"
    }
   },
   "source": [
    "model_name = \"allenai/scibert_scivocab_uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModel.from_pretrained(model_name).to(DEVICE)"
   ],
   "outputs": [],
   "execution_count": 30
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-12T18:44:00.587058Z",
     "start_time": "2025-05-12T18:43:46.011037Z"
    }
   },
   "source": [
    "reranked_knrm_query_dev_df = rerank(df_query_dev, 'knrm', DEVICE)"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "loading_doc_embeddings:   0%|          | 0/7718 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "4cdbd824bb9142f9b03c7d74f93c60ac"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "RuntimeError",
     "evalue": "MPS backend out of memory (MPS allocated: 18.15 GB, other allocations: 3.47 MB, max allowed: 18.13 GB). Tried to allocate 1.04 MB on private pool. Use PYTORCH_MPS_HIGH_WATERMARK_RATIO=0.0 to disable upper limit for memory allocations (may cause system failure).",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mRuntimeError\u001B[39m                              Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[31]\u001B[39m\u001B[32m, line 1\u001B[39m\n\u001B[32m----> \u001B[39m\u001B[32m1\u001B[39m reranked_knrm_query_dev_df = \u001B[43mrerank\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdf_query_dev\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[33;43m'\u001B[39;49m\u001B[33;43mknrm\u001B[39;49m\u001B[33;43m'\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mDEVICE\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[29]\u001B[39m\u001B[32m, line 19\u001B[39m, in \u001B[36mrerank\u001B[39m\u001B[34m(df, save_name, device, mus, sigmas, ngram_size, conv_channels)\u001B[39m\n\u001B[32m     17\u001B[39m doc_embeddings = {}\n\u001B[32m     18\u001B[39m \u001B[38;5;28;01mfor\u001B[39;00m doc_id, data \u001B[38;5;129;01min\u001B[39;00m tqdm(doc_embeddings_allenai.items(), desc=\u001B[33m\"\u001B[39m\u001B[33mloading_doc_embeddings\u001B[39m\u001B[33m\"\u001B[39m):\n\u001B[32m---> \u001B[39m\u001B[32m19\u001B[39m     emb = \u001B[43mtorch\u001B[49m\u001B[43m.\u001B[49m\u001B[43mload\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdata\u001B[49m\u001B[43m[\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mpath\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmap_location\u001B[49m\u001B[43m=\u001B[49m\u001B[43mdevice\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m     20\u001B[39m     doc_embeddings[doc_id] = emb\n\u001B[32m     22\u001B[39m \u001B[38;5;28;01mwith\u001B[39;00m torch.no_grad():\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Library/Mobile Documents/com~apple~CloudDocs/MA/Semester_2/Advanced Information Retrieval/AIR_SS25/venv/lib/python3.11/site-packages/torch/serialization.py:1516\u001B[39m, in \u001B[36mload\u001B[39m\u001B[34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001B[39m\n\u001B[32m   1514\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m weights_only:\n\u001B[32m   1515\u001B[39m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m1516\u001B[39m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43m_load\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m   1517\u001B[39m \u001B[43m            \u001B[49m\u001B[43mopened_zipfile\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1518\u001B[39m \u001B[43m            \u001B[49m\u001B[43mmap_location\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1519\u001B[39m \u001B[43m            \u001B[49m\u001B[43m_weights_only_unpickler\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1520\u001B[39m \u001B[43m            \u001B[49m\u001B[43moverall_storage\u001B[49m\u001B[43m=\u001B[49m\u001B[43moverall_storage\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1521\u001B[39m \u001B[43m            \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mpickle_load_args\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1522\u001B[39m \u001B[43m        \u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1523\u001B[39m     \u001B[38;5;28;01mexcept\u001B[39;00m pickle.UnpicklingError \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[32m   1524\u001B[39m         \u001B[38;5;28;01mraise\u001B[39;00m pickle.UnpicklingError(_get_wo_message(\u001B[38;5;28mstr\u001B[39m(e))) \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Library/Mobile Documents/com~apple~CloudDocs/MA/Semester_2/Advanced Information Retrieval/AIR_SS25/venv/lib/python3.11/site-packages/torch/serialization.py:2114\u001B[39m, in \u001B[36m_load\u001B[39m\u001B[34m(zip_file, map_location, pickle_module, pickle_file, overall_storage, **pickle_load_args)\u001B[39m\n\u001B[32m   2112\u001B[39m \u001B[38;5;28;01mglobal\u001B[39;00m _serialization_tls\n\u001B[32m   2113\u001B[39m _serialization_tls.map_location = map_location\n\u001B[32m-> \u001B[39m\u001B[32m2114\u001B[39m result = \u001B[43munpickler\u001B[49m\u001B[43m.\u001B[49m\u001B[43mload\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   2115\u001B[39m _serialization_tls.map_location = \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m   2117\u001B[39m torch._utils._validate_loaded_sparse_tensors()\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Library/Mobile Documents/com~apple~CloudDocs/MA/Semester_2/Advanced Information Retrieval/AIR_SS25/venv/lib/python3.11/site-packages/torch/_weights_only_unpickler.py:532\u001B[39m, in \u001B[36mUnpickler.load\u001B[39m\u001B[34m(self)\u001B[39m\n\u001B[32m    524\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m (\n\u001B[32m    525\u001B[39m         \u001B[38;5;28mtype\u001B[39m(pid) \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28mtuple\u001B[39m\n\u001B[32m    526\u001B[39m         \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(pid) > \u001B[32m0\u001B[39m\n\u001B[32m    527\u001B[39m         \u001B[38;5;129;01mand\u001B[39;00m torch.serialization._maybe_decode_ascii(pid[\u001B[32m0\u001B[39m]) != \u001B[33m\"\u001B[39m\u001B[33mstorage\u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m    528\u001B[39m     ):\n\u001B[32m    529\u001B[39m         \u001B[38;5;28;01mraise\u001B[39;00m UnpicklingError(\n\u001B[32m    530\u001B[39m             \u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[33mOnly persistent_load of storage is allowed, but got \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mpid[\u001B[32m0\u001B[39m]\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m\"\u001B[39m\n\u001B[32m    531\u001B[39m         )\n\u001B[32m--> \u001B[39m\u001B[32m532\u001B[39m     \u001B[38;5;28mself\u001B[39m.append(\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mpersistent_load\u001B[49m\u001B[43m(\u001B[49m\u001B[43mpid\u001B[49m\u001B[43m)\u001B[49m)\n\u001B[32m    533\u001B[39m \u001B[38;5;28;01melif\u001B[39;00m key[\u001B[32m0\u001B[39m] \u001B[38;5;129;01min\u001B[39;00m [BINGET[\u001B[32m0\u001B[39m], LONG_BINGET[\u001B[32m0\u001B[39m]]:\n\u001B[32m    534\u001B[39m     idx = (read(\u001B[32m1\u001B[39m) \u001B[38;5;28;01mif\u001B[39;00m key[\u001B[32m0\u001B[39m] == BINGET[\u001B[32m0\u001B[39m] \u001B[38;5;28;01melse\u001B[39;00m unpack(\u001B[33m\"\u001B[39m\u001B[33m<I\u001B[39m\u001B[33m\"\u001B[39m, read(\u001B[32m4\u001B[39m)))[\u001B[32m0\u001B[39m]\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Library/Mobile Documents/com~apple~CloudDocs/MA/Semester_2/Advanced Information Retrieval/AIR_SS25/venv/lib/python3.11/site-packages/torch/serialization.py:2078\u001B[39m, in \u001B[36m_load.<locals>.persistent_load\u001B[39m\u001B[34m(saved_id)\u001B[39m\n\u001B[32m   2076\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m   2077\u001B[39m     nbytes = numel * torch._utils._element_size(dtype)\n\u001B[32m-> \u001B[39m\u001B[32m2078\u001B[39m     typed_storage = \u001B[43mload_tensor\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m   2079\u001B[39m \u001B[43m        \u001B[49m\u001B[43mdtype\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mnbytes\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mkey\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m_maybe_decode_ascii\u001B[49m\u001B[43m(\u001B[49m\u001B[43mlocation\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   2080\u001B[39m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   2082\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m typed_storage\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Library/Mobile Documents/com~apple~CloudDocs/MA/Semester_2/Advanced Information Retrieval/AIR_SS25/venv/lib/python3.11/site-packages/torch/serialization.py:2044\u001B[39m, in \u001B[36m_load.<locals>.load_tensor\u001B[39m\u001B[34m(dtype, numel, key, location)\u001B[39m\n\u001B[32m   2040\u001B[39m \u001B[38;5;66;03m# TODO: Once we decide to break serialization FC, we can\u001B[39;00m\n\u001B[32m   2041\u001B[39m \u001B[38;5;66;03m# stop wrapping with TypedStorage\u001B[39;00m\n\u001B[32m   2043\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m torch._guards.detect_fake_mode(\u001B[38;5;28;01mNone\u001B[39;00m) \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m2044\u001B[39m     wrap_storage = \u001B[43mrestore_location\u001B[49m\u001B[43m(\u001B[49m\u001B[43mstorage\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlocation\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   2045\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m   2046\u001B[39m     storage._fake_device = location\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Library/Mobile Documents/com~apple~CloudDocs/MA/Semester_2/Advanced Information Retrieval/AIR_SS25/venv/lib/python3.11/site-packages/torch/serialization.py:1854\u001B[39m, in \u001B[36m_get_restore_location.<locals>.restore_location\u001B[39m\u001B[34m(storage, location)\u001B[39m\n\u001B[32m   1853\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mrestore_location\u001B[39m(storage, location):\n\u001B[32m-> \u001B[39m\u001B[32m1854\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mdefault_restore_location\u001B[49m\u001B[43m(\u001B[49m\u001B[43mstorage\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmap_location\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Library/Mobile Documents/com~apple~CloudDocs/MA/Semester_2/Advanced Information Retrieval/AIR_SS25/venv/lib/python3.11/site-packages/torch/serialization.py:698\u001B[39m, in \u001B[36mdefault_restore_location\u001B[39m\u001B[34m(storage, location)\u001B[39m\n\u001B[32m    678\u001B[39m \u001B[38;5;250m\u001B[39m\u001B[33;03m\"\"\"\u001B[39;00m\n\u001B[32m    679\u001B[39m \u001B[33;03mRestores `storage` using a deserializer function registered for the `location`.\u001B[39;00m\n\u001B[32m    680\u001B[39m \n\u001B[32m   (...)\u001B[39m\u001B[32m    695\u001B[39m \u001B[33;03m       all matching ones return `None`.\u001B[39;00m\n\u001B[32m    696\u001B[39m \u001B[33;03m\"\"\"\u001B[39;00m\n\u001B[32m    697\u001B[39m \u001B[38;5;28;01mfor\u001B[39;00m _, _, fn \u001B[38;5;129;01min\u001B[39;00m _package_registry:\n\u001B[32m--> \u001B[39m\u001B[32m698\u001B[39m     result = \u001B[43mfn\u001B[49m\u001B[43m(\u001B[49m\u001B[43mstorage\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlocation\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    699\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m result \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[32m    700\u001B[39m         \u001B[38;5;28;01mreturn\u001B[39;00m result\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Library/Mobile Documents/com~apple~CloudDocs/MA/Semester_2/Advanced Information Retrieval/AIR_SS25/venv/lib/python3.11/site-packages/torch/serialization.py:564\u001B[39m, in \u001B[36m_mps_deserialize\u001B[39m\u001B[34m(obj, location)\u001B[39m\n\u001B[32m    562\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34m_mps_deserialize\u001B[39m(obj, location):\n\u001B[32m    563\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m location.startswith(\u001B[33m\"\u001B[39m\u001B[33mmps\u001B[39m\u001B[33m\"\u001B[39m):\n\u001B[32m--> \u001B[39m\u001B[32m564\u001B[39m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mobj\u001B[49m\u001B[43m.\u001B[49m\u001B[43mmps\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Library/Mobile Documents/com~apple~CloudDocs/MA/Semester_2/Advanced Information Retrieval/AIR_SS25/venv/lib/python3.11/site-packages/torch/storage.py:268\u001B[39m, in \u001B[36m_StorageBase.mps\u001B[39m\u001B[34m(self)\u001B[39m\n\u001B[32m    266\u001B[39m \u001B[38;5;250m\u001B[39m\u001B[33;03m\"\"\"Return a MPS copy of this storage if it's not already on the MPS.\"\"\"\u001B[39;00m\n\u001B[32m    267\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m.device.type != \u001B[33m\"\u001B[39m\u001B[33mmps\u001B[39m\u001B[33m\"\u001B[39m:\n\u001B[32m--> \u001B[39m\u001B[32m268\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mtorch\u001B[49m\u001B[43m.\u001B[49m\u001B[43mUntypedStorage\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43msize\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdevice\u001B[49m\u001B[43m=\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mmps\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m)\u001B[49m.copy_(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;28;01mFalse\u001B[39;00m)\n\u001B[32m    269\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\n",
      "\u001B[31mRuntimeError\u001B[39m: MPS backend out of memory (MPS allocated: 18.15 GB, other allocations: 3.47 MB, max allowed: 18.13 GB). Tried to allocate 1.04 MB on private pool. Use PYTORCH_MPS_HIGH_WATERMARK_RATIO=0.0 to disable upper limit for memory allocations (may cause system failure)."
     ]
    }
   ],
   "execution_count": 31
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QVKBlTCZUMSc"
   },
   "source": [
    "# 5) Evaluation\n",
    "The following code evaluates the BM25 retrieval baseline on the query set using the Mean Reciprocal Rank score (MRR@5)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1742976555898,
     "user": {
      "displayName": "Yavuz",
      "userId": "01318046262282431930"
     },
     "user_tz": -60
    },
    "id": "c-vdGWXXTgjZ"
   },
   "outputs": [],
   "source": [
    "# Evaluate retrieved candidates using MRR@k\n",
    "def get_performance_mrr(data, col_gold, col_pred, list_k = [1, 5, 10]):\n",
    "    d_performance = {}\n",
    "    for k in list_k:\n",
    "        data[\"in_topx\"] = data.apply(lambda x: (1/([i for i in x[col_pred][:k]].index(x[col_gold]) + 1) if x[col_gold] in [i for i in x[col_pred][:k]] else 0), axis=1)\n",
    "        #performances.append(data[\"in_topx\"].mean())\n",
    "        d_performance[k] = data[\"in_topx\"].mean()\n",
    "    return d_performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 44,
     "status": "ok",
     "timestamp": 1742976568622,
     "user": {
      "displayName": "Yavuz",
      "userId": "01318046262282431930"
     },
     "user_tz": -60
    },
    "id": "xLX9SMg5USkH",
    "outputId": "7c414679-6486-4e08-dffe-23d8cffbddf0"
   },
   "outputs": [],
   "source": [
    "# ---- BM25 Baseline ----\n",
    "results_train = get_performance_mrr(df_query_train, 'cord_uid', 'bm25_topk')\n",
    "results_dev = get_performance_mrr(df_query_dev, 'cord_uid', 'bm25_topk')\n",
    "\n",
    "print(\"---- BM25 Baseline ----\")\n",
    "print(f\"Results on the train set: {results_train}\")\n",
    "print(f\"Results on the dev set: {results_dev}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Conv-KNRM Re-Ranking ----\n",
    "model_name = \"conv_knrm\"\n",
    "\n",
    "results_dev = get_performance_mrr(df_query_dev, 'cord_uid', f'{model_name}_topk')\n",
    "print(\"---- Re-Ranking Finetune: Conv-KNRM ----\")\n",
    "print(f\"MRR@5 on dev set: {results_dev[5]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results documentation\n",
    "\n",
    "### 1) Conv-KNRM\n",
    "Re-Ranking of top 100 BM25 results for each query:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RazcRTV84KQC"
   },
   "source": [
    "# 6) Exporting results to prepare the submission on Codalab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 39,
     "status": "ok",
     "timestamp": 1742976603546,
     "user": {
      "displayName": "Yavuz",
      "userId": "01318046262282431930"
     },
     "user_tz": -60
    },
    "id": "DFng4ocDw3Hk"
   },
   "outputs": [],
   "source": [
    "model_name = \"bm25\"\n",
    "\n",
    "df_query_dev['preds'] = df_query_dev[f'{model_name}_topk'].parallel_apply(lambda x: x[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 13,
     "status": "ok",
     "timestamp": 1742976608184,
     "user": {
      "displayName": "Yavuz",
      "userId": "01318046262282431930"
     },
     "user_tz": -60
    },
    "id": "nAVBQYh_xP8O"
   },
   "outputs": [],
   "source": [
    "df_query_dev[['post_id', 'preds']].to_csv('predictions.tsv', index=None, sep='\\t')"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
